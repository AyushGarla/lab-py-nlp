{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acf31a39",
   "metadata": {},
   "source": [
    "# **Comprehensive NLP Lab: From Preprocessing to Feature Extraction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb5fcd9",
   "metadata": {},
   "source": [
    "In this lab, you will explore a wide range of Natural Language Processing (NLP) techniques, from basic text preprocessing to advanced feature extraction and analysis. By the end of this lab, you will be able to:\n",
    "\n",
    "1. **Tokenize** and preprocess text data.\n",
    "2. Remove **stop words** and **punctuation**.\n",
    "3. Apply **stemming** and **lemmatization**.\n",
    "4. Extract features using **Bag of Words (BoW)** and **TF-IDF**.\n",
    "5. Generate **n-grams** to capture contextual information.\n",
    "6. Evaluate the impact of different preprocessing techniques on text data.\n",
    "\n",
    "Let's dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bdb53e",
   "metadata": {},
   "source": [
    "## **1. Setup the Environment**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa12605",
   "metadata": {},
   "source": [
    "Before we begin, ensure you have the necessary libraries installed. Run the following cell to install them:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd9b473",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk scikit-learn pandas matplotlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3839e6cf",
   "metadata": {},
   "source": [
    "Now, import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27aa77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52c6bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a2544b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK datasets\n",
    "#nltk.download('punkt') #\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('punkt_tab')\n",
    "#nltk.download('omw-1.4')                    # optional, for WordNet POS mappings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec67490",
   "metadata": {},
   "source": [
    "## **2. Text Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6877c3",
   "metadata": {},
   "source": [
    "### **Exercise 1: Tokenization and Stop Word Removal**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956bdaa1",
   "metadata": {},
   "source": [
    "Tokenize the following text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82fcea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Natural Language Processing (NLP) is a fascinating field of study! It involves analyzing and understanding human language.\"\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(sentence)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26303ab9",
   "metadata": {},
   "source": [
    "Remove stop words and store the result in a variable called `filtered_tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09992c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered_tokens = [\n",
    "    tok for tok in tokens\n",
    "    if tok.isalpha()                     # keep only alphabetic tokens\n",
    "    and tok.lower() not in stop_words    # exclude stop words\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14120510",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Filtered Tokens:\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f0b30d",
   "metadata": {},
   "source": [
    "### **Exercise 2: Stemming and Lemmatization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6949309",
   "metadata": {},
   "source": [
    "Apply stemming and lemmatization to the `filtered_tokens`. Compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dd22dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d063d90e",
   "metadata": {},
   "source": [
    "Apply stemming and store the result in `stemmed_tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6120c6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stemmed_tokens = [stemmer.stem(tok) for tok in filtered_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c5d353",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Stemmed Tokens:\", stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dc333a",
   "metadata": {},
   "source": [
    "Apply lemmatization and store the result in `lemmatized_tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b23234",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens1 = [lemmatizer.lemmatize(tok) for tok in filtered_tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f49cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lemmatized Tokens:\", lemmatized_tokens1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20531b9b",
   "metadata": {},
   "source": [
    "## **3. Feature Extraction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fbd840",
   "metadata": {},
   "source": [
    "### **Exercise 3: Bag of Words (BoW)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7765074",
   "metadata": {},
   "source": [
    "Use the `CountVectorizer` from `scikit-learn` to create a Bag of Words representation of the following corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c86f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"I love NLP.\",\n",
    "    \"NLP is amazing.\",\n",
    "    \"I enjoy learning new things in NLP.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45334917",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "# Step 1: Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Step 2: Fit and transform the corpus into a BoW representation\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "# Creating a DataFrame for readability\n",
    "df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "# Display \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb3b95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Bag of Words:\\n\", X.toarray())\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d79220",
   "metadata": {},
   "source": [
    "### **Exercise 4: TF-IDF**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55622108",
   "metadata": {},
   "source": [
    "Use the `TfidfVectorizer` from `scikit-learn` to create a TF-IDF representation of the same corpus. Store the result in `X_tfidf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba20ee5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Step 1: Initialize the TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Step 2: Fit and transform the corpus into a TF-IDF representation\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
    "# Create a DataFrame for readability\n",
    "df_tfidf = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "#display\n",
    "df_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0df3ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TF-IDF:\\n\", X_tfidf.toarray())\n",
    "print(\"Vocabulary:\", tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a6cee3",
   "metadata": {},
   "source": [
    "### **Exercise 5: N-grams**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c9d66d",
   "metadata": {},
   "source": [
    "Generate `bigrams (2-grams)` from the corpus using `CountVectorizer`. Store the result in `X_bigram`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de7b987",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Step 1: Initialize the CountVectorizer with ngram_range=(2, 2)\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "\n",
    "# Step 2: Fit and transform the corpus into a bigram representation\n",
    "X_bigram = bigram_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Create a DataFrame for readability\n",
    "df_bigram = pd.DataFrame(X_bigram.toarray(), columns=bigram_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display to user\n",
    "df_bigram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b210fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Bigrams:\\n\", X_bigram.toarray())\n",
    "print(\"Bigram Vocabulary:\", bigram_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7896acb",
   "metadata": {},
   "source": [
    "## **4. Advanced Exercise: Custom Preprocessing Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68072a1e",
   "metadata": {},
   "source": [
    "### **Exercise 6: Build a Custom Preprocessing Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb33268",
   "metadata": {},
   "source": [
    "Combine all the preprocessing steps (tokenization, stop word removal, punctuation removal, stemming/lemmatization) into a single function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497d534a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfff29b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "def text_preprocessing_pipeline(text):\n",
    "\n",
    "    # Step 1: Remove punctuation\n",
    "    text = text.lower()\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "\n",
    "    # Step 2: Tokenize the text\n",
    "    token = word_tokenize(text)\n",
    "\n",
    "    # Step 3: Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    rem_sw = [tok for tok in token if tok.isalpha() and tok.lower() not in stop_words]\n",
    "\n",
    "    # Step 4: Apply lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(tok) for tok in rem_sw]\n",
    "\n",
    "\n",
    "    return lemmatized_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8170a6c",
   "metadata": {},
   "source": [
    "Apply this function to the following text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1777799",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"Natural Language Processing (NLP) is a better fascinating field of study! It involves analyzing and understanding human language.\"\n",
    "\n",
    "processed_text = text_preprocessing_pipeline(text1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75b50c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Processed Text:\", processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3625df20",
   "metadata": {},
   "source": [
    "## **5. Evaluation of Preprocessing Techniques**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a666da0",
   "metadata": {},
   "source": [
    "### **Exercise 7: Compare Preprocessing Techniques**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae05a37",
   "metadata": {},
   "source": [
    "Compare the results of stemming and lemmatization on the following sentence. Store the results in `stemmed_tokens` and `lemmatized_tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e70cba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The cats are playing with the mice in the garden.\"\n",
    "import nltk\n",
    "# Step 1: Tokenize and preprocess the sentence and store the result in filtered_tokens\n",
    "from nltk.tokenize import word_tokenize\n",
    "filtered_tokens = word_tokenize(sentence)\n",
    "# Step 2: Apply stemming\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(i) for i in filtered_tokens]\n",
    "# Step 3: Apply lemmatization\n",
    "from nltk.corpus import wordnet\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Mapping POS tag to first character lemmatize() accepts\"\"\"\n",
    "    if treebank_tag.startswith('J'): return wordnet.ADJ\n",
    "    if treebank_tag.startswith('V'): return wordnet.VERB\n",
    "    if treebank_tag.startswith('N'): return wordnet.NOUN\n",
    "    if treebank_tag.startswith('R'): return wordnet.ADV\n",
    "    return wordnet.NOUN\n",
    "pos_tags = nltk.pos_tag(filtered_tokens)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [\n",
    "    lemmatizer.lemmatize(token, get_wordnet_pos(pos))\n",
    "    for token, pos in pos_tags\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125ca6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original Tokens:\", filtered_tokens)\n",
    "print(\"Stemmed Tokens:\", stemmed_tokens)\n",
    "print(\"Lemmatized Tokens:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32c465f",
   "metadata": {},
   "source": [
    "## **6. Real-World Dataset: Sentiment Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a598d9",
   "metadata": {},
   "source": [
    "### **Exercise 8: Preprocess and Analyze Tweets**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d056ab80",
   "metadata": {},
   "source": [
    "In this exercise, you will work with a real-world dataset of tweets. The dataset contains 5000 positive and 5000 negative tweets. Your task is to preprocess the tweets and extract features for sentiment analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e37f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c60819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "from nltk.corpus import twitter_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43847ae",
   "metadata": {},
   "source": [
    "Load the dataset of positive and negative tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b423ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339b8248",
   "metadata": {},
   "source": [
    "Combine them into a single list called ``all_tweets`` and create a corresponding list of labels called `labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271a4ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine into one list\n",
    "all_tweets = positive_tweets + negative_tweets\n",
    "\n",
    "# Create corresponding labels\n",
    "labels = ['positive'] * len(positive_tweets) + ['negative'] * len(negative_tweets)\n",
    "\n",
    "# Display some information\n",
    "print(f\"Total tweets: {len(all_tweets)}\")\n",
    "print(f\"Positive tweets: {len(positive_tweets)}\")\n",
    "print(f\"Negative tweets: {len(negative_tweets)}\")\n",
    "print(f\"First 5 labels: {labels[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ec3ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a sample tweet\n",
    "print(\"Sample Tweet:\", all_tweets[0])\n",
    "print(\"Label:\", labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc2a04a",
   "metadata": {},
   "source": [
    "### **Exercise 9: Preprocess Tweets**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d1e984",
   "metadata": {},
   "source": [
    "Apply the custom preprocessing pipeline to the entire dataset of tweets. Store the result in ``preprocessed_tweets``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e8c02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Apply the preprocessing pipeline to all tweets\n",
    "# your code here\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "# 1. Initialize\n",
    "tokenizer   = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "stop_words  = set(stopwords.words('english'))\n",
    "stemmer     = PorterStemmer()\n",
    "\n",
    "# 2. Define your pipeline function\n",
    "def preprocess_tweet(tweet):\n",
    "    # a) Tokenize & lowercase (TweetTokenizer already lowercases if preserve_case=False)\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "    # b) Remove stopwords, punctuation, and non-alphabetic tokens\n",
    "    tokens = [t for t in tokens if t.isalpha() and t not in stop_words]\n",
    "    # c) Stem each token\n",
    "    stems = [stemmer.stem(t) for t in tokens]\n",
    "    return stems\n",
    "# 3. Apply to all tweets\n",
    "preprocessed_tweets = [preprocess_tweet(t) for t in all_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeef254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a sample preprocessed tweet\n",
    "print(\"Preprocessed Tweets Sample:\", preprocessed_tweets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8658daf",
   "metadata": {},
   "source": [
    "### **Exercise 10: Feature Extraction on Tweets**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381f161c",
   "metadata": {},
   "source": [
    "Extract features from the preprocessed tweets using **Bag of Words** and **TF-IDF**. Store the results in ``X_bow`` and ``X_tfidf``, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee42f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# If preprocessed_tweets is a list of token lists, join them back into strings:\n",
    "documents = [\" \".join(tokens) for tokens in preprocessed_tweets]\n",
    "# Step 1: Create a Bag of Words representation\n",
    "\n",
    "# 1) Bag-of-Words\n",
    "bow_vectorizer = CountVectorizer()\n",
    "X_bow = bow_vectorizer.fit_transform(documents)\n",
    "# X_bow is a scipy sparse matrix of shape (n_samples, n_features)\n",
    "\n",
    "# Step 2: Create a TF-IDF representation\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "# X_tfidf is a scipy sparse matrix of the same shape\n",
    "\n",
    "print(\"BoW shape:   \", X_bow.shape)\n",
    "print(\"TF-IDF shape:\", X_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4912687c",
   "metadata": {},
   "source": [
    "## **7. Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948ed4ac",
   "metadata": {},
   "source": [
    "In this lab, you explored a wide range of NLP techniques, from basic text preprocessing to advanced feature extraction and analysis. You also worked with a real-world dataset of tweets and applied your knowledge to preprocess and extract features for sentiment analysis.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ironhack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
